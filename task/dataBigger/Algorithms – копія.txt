Chapter 1:
Traveling salesman problem story.

There is a fundamental difference between algorithms, which always produce a correct result, and heuristics, which may usually do a good job but without providing any guarantee.

Reasonable-looking algorithms can easily be incorrect. Algorithm correctness is a property that must be carefully demonstrated.

The heart of any algorithm is an idea. If your idea is not clearly revealed when you express an algorithm, then you are using too low-level anotation to describe it.

An important and honorable technique in algorithm design is to narrow the set of allowable instances until there is a correct and efficient algorithm. For example, we can restrict a graph problem from general graphs down to trees, or a geometric problem from two dimensions down to one.

Searching for counterexamples is the best way to disprove the correctness of a heuristic.

Mathematical induction is usually the right way to verify the correctness of a recursive or incremental insertion algorithm.

Modeling your application in terms of well-defined struc- tures and algorithms is the most important single step towards a solution.

Chapter 2:
Our two most important tools are (1) the RAM model of computation and (2) the asymptotic analysis of worst-case complexity.

"Every model has a size range over which it is useful. Take, for example, the model that the Earth is flat. You might argue that this is a bad model, since it has been fairly well established that the Earth is in fact round. But, when laying the foundation of a house, the flat Earth model is sufficiently accurate that it can be reliably used."

Algorithms can be understood and studied in a language- and machine-independent manner.

Although esoteric functions arise in advanced algorithm analysis, a small variety of time complexities suffice and account for most algorithms that are widely used in practice.

What is the height h of a rooted binary tree with n leaf nodes?  n leaves = 2^h thus, h = log n base 2

What if we generalize to trees that have d children? h = log d base 2

There are two bit patterns of length 1 (0 and 1) and four of length 2 (00, 01, 10, and 11). How many bits w do we need to represent any one of n different possibilities, be it one of n items or the integers from 1 to n? w = log n base 2. as 2^w = n.

function power(a,n)
    if (n = 0) return(1)
    x = power(a, ⌊n/2⌋)
    if (n is even) then return(x2)
    else return(a × x2)

Logarithms arise whenever things are repeatedly halved or doubled.

How many queries does binary search take on the million-name Manhat- tan phone book if each split was 1/3 to 2/3 instead of 1/2 to 1/2?
When performing binary searches in a telephone book, how important is it that each query split the book exactly in half? Not much. For the Manhattan telephone book, we now use log3/2(1, 000, 000) ≈ 35 queries in the worst case, not a significant change from log2(1, 000, 000) ≈ 20.

The power of binary search comes from its logarithmic complexity, not the base of the log.

Dynamic memory allocation provides us with flexibility on how and where we use our limited storage resources.

Chapter 3:

Data structure design must balance all the different operations it supports. The fastest data structure to support both operations A and B may well not be the fastest structure to support either operation A or B.

Data structures can be neatly classified as either contiguous or linked, depending upon whether they are based on arrays or pointers:
- Contiguously-allocated structures are composed of single slabs of memory, and include arrays, matrices, heaps, and hash tables.
- Linked data structures are composed of distinct chunks of memory bound together by pointers, and include lists, trees, and graph adjacency lists.

One final thought about these fundamental structures is that they can be thought of as recursive objects:
• Lists – Chopping the first element off a linked list leaves a smaller linked list. This same argument works for strings, since removing characters from string leaves a string. Lists are recursive objects.
• Arrays – Splitting the first k elements off of an n element array gives two smaller arrays, of size k and n−k, respectively. Arrays are recursive objects.
This insight leads to simpler list processing, and efficient divide-and-conquer algorithms such as quicksort and binary search.

We use the term container to denote a data structure that permits storage and retrieval of data items independent of content. By contrast, dictionaries are abstract data types that retrieve based on key values or content.

Containers are distinguished by the particular retrieval order they support:
Stack(Push/POP) & Queue(EnQueue/DeQueue).

Dictionary operations:
Search(D,x) + Insert(D,x) + Delete(D,x) + Max(D) + Min(D + Successor(D,x) + Predecessor(D,x)

Unsorted/Sorted Array
Search(L, k) O(n)/O(log n){Binary Search}
Insert(L, x) O(1)/O(n)
Delete(L, x) O(1)*{Use SWAP and delete algo for constant time}/O(n)
Successor(L, x){Element which is >= x} O(n)/O(1)
Predecessor(L, x){Element which is <= x} O(n)/O(1)
Minimum(L) O(n)/O(1)
Maximum(L) O(n)/O(1)

Dictionary operation
Singly unsorted linked list
Search(L, k) O(n)
Insert(L, x) O(1)
Delete(L, x) O(n)*
Successor(L, x) O(n)
Predecessor(L, x) O(n)
Minimum(L) O(n)
Maximum(L) O(n)

Double unsorted linked list
Search(L, k) O(n)
Insert(L, x) O(1)
Delete(L, x) O(1)
Successor(L, x) O(n)
Predecessor(L, x) O(n)
Minimum(L) O(n)
Maximum(L) O(n)

Singly sorted linked list
Search(L, k) O(n)
Insert(L, x) O(n)
Delete(L, x) O(n)*
Successor(L, x) O(1)
Predecessor(L, x) O(n)*
Minimum(L) O(1)
Maximum(L) O(1)*

Doubly sorted linked list
Search(L, k) O(n)
Insert(L, x) O(n)
Delete(L, x) O(1)
Successor(L, x) O(1)
Predecessor(L, x) O(1)
Minimum(L) O(1)
Maximum(L) O(1)

Binary Search Trees.
Balance Search Trees. Sophisticated balanced binary search tree data structures have been developed that guarantee the height of the tree always to be O(log n). Therefore, all dictionary operations (insert, delete, query) take O(log n) time each.


Picking the wrong data structure for the job can be disastrous in terms of performance. Identifying the very best data structure is usually not as critical, because there can be several choices that perform similarly.

Priority queues are data structures that provide more flexibility than simple sorting, because they allow new elements to enter a system at arbitrary intervals. It is much more cost-effective to insert a new job into a priority queue than to re-sort everything on each such arrival.
The basic priority queue supports three primary operations:
• Insert(Q,x)– Given an item x with key k, insert it into the priority queue Q.
• Find-Minimum(Q) or Find-Maximum(Q)– Return a pointer to the item whose key value is smaller (larger) than any other key in the priority queue Q.
• Delete-Minimum(Q) or Delete-Maximum(Q)– Remove the item from the priority queue Q whose key is minimum (maximum).

Building algorithms around data structures such as dictionaries and priority queues leads to both clean structure and good performance.

Problem: What is the worst-case time complexity of the three basic priority queue operations (insert, find-minimum, and delete-minimum) when the basic data structure is
• An unsorted array. O(1), O(n), O(n)
• A sorted array. O(n), O(1), O(1)
• A balanced binary search tree. O(log n), O(log n), O(log n)

Hash tables are a very practical way to maintain a dictionary. They exploit the fact that looking an item up in an array takes constant time once you have its index. A hash function is a mathematical function that maps keys to integers. 

Stop and Think: 
    Exploiting Balanced Search Trees
    Problem: You are given the task of reading n numbers and then printing them out in sorted order. Suppose you have access to a balanced dictionary data structure, which supports the operations search, insert, delete, minimum, maximum, successor, and predecessor each in O(log n) time.
    1. How can you sort in O(n log n) time using only insert and in-order traversal?
    2. How can you sort in O(nlogn) time using only minimum, successor, and
    insert?
    3. How can you sort in O(nlogn) time using only minimum, insert, delete, search?
Each of these algorithms does a linear number of logarithmic-time operations, and hence runs in O(nlogn) time. 

The key to exploiting balanced binary search trees is using them as black boxes.